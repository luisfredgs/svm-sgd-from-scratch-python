# Stochastic Gradient Descent (SGD)

Stochastic gradient descent (SGD) in contrast to batch version performs a parameter update for each training example $x^{(i)}$ and label $y^{(i)}$:

<img src="https://render.githubusercontent.com/render/math?math=\theta = \theta - \eta*\nabla L(y^i, f(x^i, \theta))">

Since SGD uses only one sample per iteration, the computation complexity for each iteration is $O(D)$ where $D$ is the number of features. When the number of samples $N$ is large, the update rate for each iteration of SGD is much faster than that of batch gradient descent.

SGD increases the overall optimization efficiency at the expense of more iterations, but the increased iteration number is insignificant compared with the high computation complexity caused by large numbers of samples. It is possible to use only thousands of samples overall to get the optimal solution even when the sample size is hundreds of thousands. Therefore, compared with batch methods, SGD can effectively reduce the computational complexity and accelerate convergence

SGDâ€™s oscillations introduced by random selection enables it to jump to new and potentially better local minima. On the other hand, the gradient direction oscillates, which may lead to the problem of non-converging - as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent. Nevertheless, further optimization algorithms based on variance reduction were proposed to improve the convergence rate.

![Stochastic Gradient Descent](sgd_animation.gif)


## Requirements to run the code

* Matplotlib
* Numpy
* Scikit-learn

## Usage

```$ python train_svm.py --epochs=100 --alpha=0.000001 --regularization_strength=10000.0```

# Results

## Console output

```
Training started...
Epoch is: 1 and Cost is: 754407.8195154738
Epoch is: 2 and Cost is: 708819.7232919635
Epoch is: 4 and Cost is: 617653.7651782469
Epoch is: 8 and Cost is: 516230.94639430585
Epoch is: 16 and Cost is: 349757.12685772
Epoch is: 32 and Cost is: 188548.63399060527
Epoch is: 64 and Cost is: 135125.1178263414
Epoch is: 128 and Cost is: 125230.5426937045
Epoch is: 256 and Cost is: 123151.89976179479
Epoch is: 512 and Cost is: 122311.55988889028
Training finished.
Testing...
accuracy on test dataset: 0.95
```
## Decision boundaries

![Decision boundaries](decision_boundaries.png)

## Tranining loss

![Tranining loss](loss.png)